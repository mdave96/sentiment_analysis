{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "kqeHZ00Q8KEs",
        "outputId": "f6e6cad2-dd09-4c17-f5d3-4715717e8ed0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz            NLP_Assignment-1_Malhar.ipynb  train_data.csv\n",
            "Assignment-3_Imdb_RNN.ipynb  NLP_Assignment-2.ipynb\n",
            "\u001b[0m\u001b[01;34mdata\u001b[0m/                        test_data.csv\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "7TePg-fm9HRe",
        "outputId": "6a970b0c-f85a-4972-db6d-d303bdf427d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import time\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF7Qj70MgY8M"
      },
      "source": [
        "## Pre-Processing the Data\n",
        "Each file is read and review is added to a dataframe.<br>\n",
        "There are two dataframes - train_data and test_data - both have 25000 reviews each with first 12500 of them being negative reviews (that's the order in which they were appended).<br>\n",
        "The dataframe was made on my local python environment instead of Colab because somehow, Colab was taking 10x as longer as local python did. This dataframe is then uploaded on Colab and accessed here.<br><br>\n",
        "Below is the code for making the dataframe from individual files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBTJIsUmgKY3"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "train, test = [], []\n",
        "\n",
        "for filename in glob.glob(\"train/neg/*.txt\"):\n",
        "  train.append(filename)\n",
        "for filename in glob.glob(\"train/pos/*.txt\"):\n",
        "  train.append(filename)\n",
        "for filename in glob.glob(\"test/neg/*.txt\"):\n",
        "  test.append(filename)\n",
        "for filename in glob.glob(\"test/pos/*.txt\"):\n",
        "  test.append(filename)\n",
        "\n",
        "train_data = pd.DataFrame(columns=['review'])\n",
        "for i in range(len(train)):\n",
        "  with open(train[i],'r',encoding='utf-8') as fil:\n",
        "    data = fil.read()\n",
        "  train_data = train_data.append({'review':data},ignore_index=True)\n",
        "  if i%1000 == 0:\n",
        "    print('Iteration:',i)\n",
        "train_data.to_csv('train_data.csv',index=False)\n",
        "\n",
        "test_data = pd.DataFrame(columns=['review'])\n",
        "for j in range(len(test)):\n",
        "  with open(test[j],'r',encoding='utf-8') as fil_1:\n",
        "    data_1 = fil_1.read()\n",
        "  test_data = test_data.append({'review':data_1},ignore_index=True)\n",
        "  if j%1000 == 0:\n",
        "    print('Iteration:',j)\n",
        "test_data.to_csv('test_data.csv',index=False)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNG25gJyhcxB"
      },
      "source": [
        "Once the dataframes are ready, a new column 'target' is defined that indicates whether the review is negative (0) or positive (1). The dataframe is then shuffled by rows because presently top 12500 are all negative and rest all positive.<br><br>\n",
        "Further, some regex code is written to clean up the review.<br><br>\n",
        "And then NLTK stopwords list is used to remove the stopwords from reviews. A couple of stopwords are removed from the stopwords list since they might play a crucial role in classifying reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAmmXFwbQoaD"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('train_data.csv')\n",
        "test_data = pd.read_csv('test_data.csv')\n",
        "\n",
        "train_data['target'] = np.asarray([0 if i < 12500 else 1 for i in range(25000)])\n",
        "test_data['target'] = np.asarray([0 if i < 12500 else 1 for i in range(25000)])\n",
        "\n",
        "train_data = shuffle(train_data).reset_index(drop=True)\n",
        "test_data = shuffle(test_data).reset_index(drop=True)\n",
        "\n",
        "train_list = list(train_data['review'])\n",
        "test_list = list(test_data['review'])\n",
        "\n",
        "y_train = np.asarray(train_data['target'])\n",
        "y_test = np.asarray(test_data['target'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_jPkH89RHoi"
      },
      "outputs": [],
      "source": [
        "# Regex expressions to clean the reviews taken from a blog\n",
        "REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
        "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
        "\n",
        "def clean(reviews):\n",
        "    reviews = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in reviews]\n",
        "    reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n",
        "    return reviews\n",
        "\n",
        "train_list = clean(train_list)\n",
        "test_list = clean(test_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AK8mKJfRHin"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(tokens, stopwords_list):\n",
        "    result = []\n",
        "    for word in tokens:\n",
        "        if word not in stopwords_list:\n",
        "            result.append(word)\n",
        "    return result\n",
        "\n",
        "stopwords_list = stopwords.words('english')\n",
        "stopwords_list.remove('not')\n",
        "stopwords_list.remove('very')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO5x7V-7mTq1"
      },
      "source": [
        "## Building vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wiy0rgfiRvK9"
      },
      "outputs": [],
      "source": [
        "# Build vocabulary from training data\n",
        "# Build dictionary of all words in training data, sort it by its frequency and\n",
        "# take top (say) 1000 or 2000 values to be the vocabulary.\n",
        "\n",
        "word_dic = {}\n",
        "X_train = []\n",
        "\n",
        "for review in train_list:\n",
        "  tokens = review.split()\n",
        "  tokens = remove_stopwords(tokens, stopwords_list)\n",
        "  X_train.append(tokens)\n",
        "  for token in tokens:\n",
        "    if token in word_dic.keys():\n",
        "      word_dic[token] += 1\n",
        "    else:\n",
        "      word_dic[token] = 1\n",
        "\n",
        "vocab_size = 1500\n",
        "words_sorted = [k for k, v in sorted(word_dic.items(), key=lambda item: item[1], reverse=True)]\n",
        "vocab = words_sorted[:vocab_size]\n",
        "\n",
        "# Make word to index and index to word dictinoaries from vocabulary for further use\n",
        "word2idx = {v:i for i,v in enumerate(vocab)}\n",
        "idx2word = {i:v for v,i in word2idx.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UY5oXKpGRvDR"
      },
      "outputs": [],
      "source": [
        "# Prepare the training and test dataset with reviews containing words only from the vocabulary.\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "  X_train[i] = [word2idx[word] for word in X_train[i] if word in vocab]\n",
        "\n",
        "X_test = []\n",
        "for review in test_list:\n",
        "  tokens = review.split()\n",
        "  temp = [word2idx[word] for word in tokens if word in vocab]\n",
        "  X_test.append(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syMGqGfBWYMq"
      },
      "outputs": [],
      "source": [
        "# Function for encoding word indexes to one hot vectors that will be input for RNN\n",
        "\n",
        "def one_hot_encode(idx, vocab_size):\n",
        "  one_hot = np.zeros((vocab_size))\n",
        "  one_hot[idx] = 1\n",
        "  return one_hot\n",
        "\n",
        "# Initially, One-hot vectors were created and saved here itself (in envrionment), before passing them to RNN\n",
        "# But this ate up all the RAM of Colab for vocab_size > 800.\n",
        "# So instead, now the one hot vectors are defined one at a time in the rnn() function defined a few blocks below\n",
        "# This creates the one hot vector, uses it for forward and backward pass and then throws it out.\n",
        "'''\n",
        "for i in range(len(X_train)):\n",
        "  X_train[i] = np.array([one_hot_encode(idx,vocab_size) for idx in X_train[i]])\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "  X_test[i] = np.array([one_hot_encode(idx,vocab_size) for idx in X_test[i]])\n",
        "'''\n",
        "X_train = np.asarray(X_train)\n",
        "X_test = np.asarray(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ecWN7RqQ8HW"
      },
      "source": [
        "## Numpy RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86rfrp4ccSkx"
      },
      "source": [
        "The RNN structure followed here is as follows : <br><br>\n",
        "X - input (array of reviews) ; y - output (0 or 1)<br>\n",
        "Hidden state - h<sub>t</sub><br>\n",
        "h<sub>t</sub> = tanh(U.X<sub>t</sub> + V.h<sub>t-1</sub> + b1) <br>\n",
        "O = W.h<sub>T</sub> + b2 <br>\n",
        "y_hat = sigmoid(O)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvMLmRCHQ-wV"
      },
      "outputs": [],
      "source": [
        "# Sigmoid activation function to be used on output layer\n",
        "def sigmoid(Z):\n",
        "  return 1 / (1 + np.exp(-Z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxz_koRTTBjc"
      },
      "outputs": [],
      "source": [
        "# Initializing weights for one hidden layer RNN\n",
        "# Initializing weights to random normal gave better results than initializing with zeros. (Bias excluded)\n",
        "def init_weights(nh, vocab_size):\n",
        "  U = np.random.randn(nh,vocab_size)\n",
        "  V = np.random.randn(nh,nh)\n",
        "  W = np.random.randn(1,nh)\n",
        "  b1 = np.zeros((nh,1))\n",
        "  b2 = np.zeros((1,1))\n",
        "  return U,V,W,b1,b2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hZ5RexLW3Bo"
      },
      "outputs": [],
      "source": [
        "# Forward pass is carried out review by review.\n",
        "# For a single review, hidden states (hts) are calculated for each word in the review by looping over it\n",
        "# And finally class probabilities are calculated as sigmoid of\n",
        "# last hidden states' value together with weight and bias.\n",
        "\n",
        "def forward_pass(input_review,weights):\n",
        "  U,V,W,b1,b2 = weights\n",
        "  hts = []\n",
        "  ht = np.zeros((V.shape[0], 1))\n",
        "  for t in range(len(input_review)):\n",
        "    ht = np.tanh(np.dot(U,input_review[t].reshape(-1,1)) + np.dot(V,ht) + b1)\n",
        "    hts.append(ht)\n",
        "  try:\n",
        "    O = np.dot(W,hts[-1]) + b2\n",
        "  except:\n",
        "    O = np.dot(W,ht) + b2\n",
        "  y_hat = sigmoid(O)\n",
        "  return y_hat, hts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2cE0HemBKNr"
      },
      "outputs": [],
      "source": [
        "# Binary Crossentropy loss\n",
        "def loss_fn(y,y_hat):\n",
        "  loss = -(y*np.log(y_hat) + (1-y)*np.log(1-y_hat))\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x94goIhU9dtB"
      },
      "outputs": [],
      "source": [
        "# Function to clip gradients to stop them exploding.\n",
        "# Implemented as described in the RNN class.\n",
        "# Clip the gradient if gradient norm is greater than a set theshold value.\n",
        "# Threshold value of 0.5 worked best\n",
        "\n",
        "def clip(gradient,threshold=0.5):\n",
        "  gradient_norm = np.sqrt(np.sum(gradient**2))\n",
        "  if gradient_norm > threshold:\n",
        "    gradient = (threshold / gradient_norm) * gradient\n",
        "  return gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8C9Z1u80fMHN"
      },
      "outputs": [],
      "source": [
        "# Backpropagation of the network.\n",
        "# Notation : dX represents the gradient of parameter X.\n",
        "# Here, parameters W and b2 are used only once and at the last time point because this is a 'many to one' type of RNN\n",
        "# Hence, gradients of W and b2 are calculated outside the loop.\n",
        "# First iteration of gradient is done outside of loop because of different nature of values it requires (coz of many to one type)\n",
        "# Then, across the time (words), gradient values are added accordingly.\n",
        "def backprop(input_review,y,y_hat,hts,weights):\n",
        "  U,V,W,b1,b2 = weights\n",
        "  dU,dV,dW,db1,db2 = np.zeros(U.shape), np.zeros(V.shape), np.zeros(W.shape), np.zeros(b1.shape), np.zeros(b2.shape)\n",
        "\n",
        "  dO = y_hat - y\n",
        "  dW = np.dot(dO,hts[-1].T)\n",
        "  db2 = dO\n",
        "  dht = np.dot(W.T,dO)\n",
        "  db1 += dht * (1-hts[-1]**2)\n",
        "  dV += np.dot(dht * (1-hts[-1]**2),hts[-2].T)\n",
        "  dU += np.dot(dht * (1-hts[-1]**2),input_review[-1].reshape(-1,1).T)\n",
        "  for t in reversed(range(len(hts)-1)):\n",
        "    dht = np.dot(V.T, dht * (1-hts[t+1]**2))\n",
        "    db1 += dht * (1-hts[t]**2)\n",
        "    dV += np.dot(dht * (1-hts[t]**2),hts[t-1].T)\n",
        "    dU += np.dot(dht * (1-hts[t]**2),input_review[t-1].reshape(-1,1).T)\n",
        "\n",
        "  gradients = [dU,dV,dW,db1,db2]\n",
        "  for g in range(len(gradients)):\n",
        "    gradients[g] = clip(gradients[g])\n",
        "    \n",
        "  return tuple(gradients)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiuFD_6Y94nP"
      },
      "outputs": [],
      "source": [
        "def update_weights(weights, gradients, learning_rate):\n",
        "  U,V,W,b1,b2 = weights\n",
        "  dU,dV,dW,db1,db2 = gradients\n",
        "  U_new = U - learning_rate * dU\n",
        "  V_new = V - learning_rate * dV\n",
        "  W_new = W - learning_rate * dW\n",
        "  b1_new = b1 - learning_rate * db1\n",
        "  b2_new = b2 - learning_rate * db2\n",
        "  new_weights = U_new, V_new, W_new, b1_new, b2_new\n",
        "  return new_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDqKnbrEBEUU"
      },
      "outputs": [],
      "source": [
        "# The final RNN model\n",
        "# Update of weights is made after each review\n",
        "# The model returns the trained weights\n",
        "def rnn_model(X,Y,nh,num_epochs,vocab_size,learning_rate=0.01):\n",
        "  weights = init_weights(nh,vocab_size)\n",
        "  loss = []\n",
        "  for itr in range(num_epochs):\n",
        "    for i in range(X.shape[0]):\n",
        "      x = np.array([one_hot_encode(idx,vocab_size) for idx in X[i]])\n",
        "      y = Y[i]\n",
        "      y_hat, hts = forward_pass(x,weights)\n",
        "      loss.append(loss_fn(y,y_hat))\n",
        "      gradients = backprop(x,y,y_hat,hts,weights)\n",
        "      weights = update_weights(weights,gradients,learning_rate)\n",
        "  return weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSHGFu8KFtp9"
      },
      "outputs": [],
      "source": [
        "def predict(X,weights,learning_rate=0.01):\n",
        "  y_pred = []\n",
        "  for i in range(X.shape[0]):\n",
        "    x = np.array([one_hot_encode(idx,vocab_size) for idx in X[i]])\n",
        "    y_hat, _ = forward_pass(x, weights)\n",
        "    if y_hat < 0.5:\n",
        "      y_pred.append(0)\n",
        "    else:\n",
        "      y_pred.append(1)\n",
        "  y_pred = np.asarray(y_pred)\n",
        "  return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HB6-y3uFH7Wk"
      },
      "outputs": [],
      "source": [
        "def accuracy(Y,Y_pred):\n",
        "  acc = np.mean(Y == Y_pred)\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Hr69NsSKIQ8X",
        "outputId": "555ef9c5-52a6-47d7-e8f0-67e7d3b4270e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It took the model 1998 seconds to run\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "weights = rnn_model(X_train,y_train,3,20,vocab_size,0.06)\n",
        "end_time = time.time()\n",
        "print('It took the model %d seconds to run' % int(end_time - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "_QKMDnf3gjwh",
        "outputId": "1f97cc08-9890-41a0-e11a-a70b55186f11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing accuracy : 0.763\n"
          ]
        }
      ],
      "source": [
        "Y_pred = predict(X_test, weights)\n",
        "print('Testing accuracy : %.3f' % accuracy(y_test, Y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "3SasuT6cV27g",
        "outputId": "c6f8ce9e-c917-4ed6-82ca-9a713a8a445c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training accuracy : 0.777\n"
          ]
        }
      ],
      "source": [
        "Y_pred = predict(X_train, weights)\n",
        "print('Training accuracy : %.3f' % accuracy(y_train, Y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpiY4lNSbnwn"
      },
      "source": [
        "So, the accuracy on test set is 76.3%.<br><br>\n",
        "Different combinations of vocab_size, hidden layer size and learning rate were tried and this particular combination gave the best result."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
